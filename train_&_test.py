# -*- coding: utf-8 -*-
"""Train & Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GqIG9MDID9RINhouz7I2K91-uSXrJLoQ
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
from torchvision import datasets
from torchvision.datasets import CIFAR10
from torch.nn import DataParallel
import torchvision.transforms as transforms
import torch.optim as optim
import torch.nn as nn
import math
import os
import torch.nn.functional as F





# Build model, define loss function and optimizer and cuda function
model = RCNN(3, 10, 96)
os.environ["CUDA_VISIBLE_DEVICES"] = "0, 1"
model.cuda()
model = nn.DataParallel(model)
GPU_COUNT = torch.cuda.device_count()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = 1e-2)    
DATADIR = "/content/data/dataset" # Data path mounted on Google Drive



# loading data: apply augmentation, convert to tensor and normalize
trainloader, testloader = load_data(DATADIR,64)
def load_data(datadir, batch_size):
    transform_train = transforms.Compose([
        transforms.RandomCrop(24),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])
    transform_test = transforms.Compose([
        transforms.TenCrop(24),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),
        transforms.Lambda(lambda crops: torch.stack([transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(crop) for crop in crops]))])
    trainset = CIFAR10(datadir, transform = transform_train, train = True, download=True)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size , shuffle = True)
    testset = CIFAR10(datadir, transform = transform_test, train = False, download=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size , shuffle = False)
    return trainloader, testloader


# training  data

correct, total = 0, 0
train_loss, counter = 0, 0
EPOCH = 15

for epoch in range(0, EPOCH):
        # iterating over all data 
      for data in trainloader:
            model.train()

            # get the inputs and pass it to cuda for parallel processing
            inputs, labels = data
            inputs = inputs.cuda()
            labels = labels.cuda()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # counting the loss and accuracy on training dataset
            _, predicted = torch.max(outputs.data, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            train_loss += loss.item()
            counter += 1
            # get acc,loss on trainset
            acc = correct / total
            train_loss /= counter
      
# test data 
test_loss, test_acc = testdata(model, testloader, criterion)
def testdata(model, testloader, criterion):

    model.eval()
    correct, total = 0, 0
    loss, counter = 0, 0
    with torch.no_grad():
        for (images, labels) in testloader:

            images = images.cuda()
            labels = labels.cuda()
            bs, ncrops, c, h, w = images.size()
            outputs = model(images.view(-1, c, h, w))
            result_avg = outputs.view(bs, ncrops, -1).mean(1)
            _, predicted = torch.max(result_avg.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            loss += criterion(result_avg, labels).item()
            counter += 1

    return loss / counter, correct / total

print('training accuracy: %.4f  testing accuracy: %.4f'
# %( acc, test_acc))